# [Machine Learning](../../README.md)
### Platforms
- [Amazon Sagemaker](https://aws.amazon.com/sagemaker/)
- [Google AutoML](https://cloud.google.com/automl/)

### Libraries
- TensorFlow
- Pytorch
- Caffe
- [Caffe2](https://caffe2.ai/)

### Hardware
- [Nvidia Jetson Hardware](https://developer.nvidia.com/embedded/develop/hardware)

### Other
- [Neural Net Visualization](https://www.graphcore.ai/posts/what-does-machine-learning-look-like)


### Learning
- [Google AI](https://ai.google/)
- [K Means Clustering](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)
- [CNN Architectures](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)
- [Intro to Convolutional Neural Networks for Visual Recognition (Youtube: Stanford U)](https://www.youtube.com/watch?v=vT1JzLTH4G4)
- [Stanford General Machine Learning Overview](http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/)
- [Deep Learning Book](http://www.deeplearningbook.org/)

Although AutoML seems to be a one-stop-shop where you don't really need to learn deep learning to use it, the open source community has been at work as well and have created [Auto Keras](https://autokeras.com/). It performs the same "NAS" (*Neural Architecture Search*) but is open source. Read more [here](https://towardsdatascience.com/autokeras-the-killer-of-googles-automl-9e84c552a319).

[This](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9) article and [this one](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)  answers some questions on Batch Size, vs Epoch, vs Iterations.

##### Google AI Class
- [Intro To ML by Google](https://developers.google.com/machine-learning/crash-course/ml-intro)

Some ML Terminology:
- Label (what we want to predict)
- Feature (Inputs)
- Unlabeled Example (Input requiring understanding)
- Model (Creates the map between features and labels)

Linear Regression: `y=wx+b`
- W = weight
- B = Bias

L2 Loss = SUM[y-prediction(x)]^2

y' = b + w1x1 + w2x2 + w3x3 ...

Mean Square Error (MSE) is the L2 loss summed up, and divided by the number of examples.


##### TensorFlow & Keras
- [TensorFlow](https://www.tensorflow.org/)
- [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)
- [Keras](https://keras.io/)
- [Keras Image Preprocessing](https://keras.io/preprocessing/image/)
- [Python Keras Tutorial](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/)

To start tensorboard, use the following command:

`tensorboard --logdir=<directory with your logs>`

The LeNet Architecture is fairly easy to get started with. Follow along with this tutorial: [Simple LeNet Architecture](https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/)

[This tutorial](https://www.pyimagesearch.com/2017/12/11/image-classification-with-keras-and-deep-learning/) is also pretty good and expands on the previous one. Build your very own "Santa" "not Santa" model!

Building a model where you have a limited dataset can be difficult. [This tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) covers how to use image pre-processing with Keras to generate a model that is pretty accurate. It also covers some detail on using pre-trained models to improve your accuracy.

You can stop the learning early based on a set of metrics. [This Kaggle](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics?scriptVersionId=4658363) competition uses it. This is one of the [Keras Callbacks](https://keras.io/callbacks/) that can be used during training.

[This](https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/) tutorial walks through steps to create a multi-label multi-class model so that each image could result with more than one end label.
